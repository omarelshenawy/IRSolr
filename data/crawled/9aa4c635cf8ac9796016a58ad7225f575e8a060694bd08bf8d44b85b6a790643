{"add":{"doc":{"id":"9aa4c635cf8ac9796016a58ad7225f575e8a060694bd08bf8d44b85b6a790643","url":"https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/JILA%27s_strontium_optical_atomic_clock.jpg/280px-JILA%27s_strontium_optical_atomic_clock.jpg","previous":["The idea of using atomic transitions to measure time was first suggested by Lord Kelvin in 1879.[3] Magnetic resonance, developed in the 1930s by Isidor Rabi, became the practical method for doing this.[4] In 1945, Rabi first publicly suggested that atomic beam magnetic resonance might be used as the basis of a clock.[5] The first atomic clock was an ammonia maser device built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). It was less accurate than existing quartz clocks, but served to demonstrate the concept.[6] The first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen in 1955 at the National Physical Laboratory in the UK.[7] Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale ephemeris time (ET).[8] This led to the internationally agreed definition of the latest SI second being based on atomic time. Equality of the ET second with the (atomic clock) SI second has been verified to within 1 part in 1010.[9] The SI second thus inherits the effect of decisions by the original designers of the ephemeris time scale, determining the length of the ET second."],"after":["Since the beginning of development in the 1950s, atomic clocks have been based on the hyperfine transitions in hydrogen-1, cesium-133, and rubidium-87. The first commercial atomic clock was the Atomichron, manufactured by the National Company. More than 50 were sold between 1956 and 1960. This bulky and expensive instrument was subsequently replaced by much smaller rack-mountable devices, such as the Hewlett-Packard model 5060 caesium frequency standard, released in 1964.[4]"]}}}