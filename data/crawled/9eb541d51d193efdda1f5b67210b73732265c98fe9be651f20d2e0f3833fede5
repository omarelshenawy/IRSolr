{"add":{"doc":{"id":"9eb541d51d193efdda1f5b67210b73732265c98fe9be651f20d2e0f3833fede5","url":"https://upload.wikimedia.org/math/4/f/0/4f0080f2c78d5b39d6f8ce8dfa076f8e.png","previous":["Suppose one transmits 1000 bits (0s and 1s). If the value of each these bits is known to the receiver (has a specific value with certainty) ahead of transmission, it is clear that no information is transmitted. If, however, each bit is independently equally likely to be 0 or 1, 1000 shannons of information (also often called bits, in the information theoretic sense) have been transmitted. Between these two extremes, information can be quantified as follows. If is the set of all messages that could be, and is the probability of some , then the entropy, , of is defined:[8]"],"after":["(Here, is the self-information, which is the entropy contribution of an individual message, and is the expected value.) A property of entropy is that it is maximized when all the messages in the message space are equiprobable ,—i.e., most unpredictable—in which case ."]}}}