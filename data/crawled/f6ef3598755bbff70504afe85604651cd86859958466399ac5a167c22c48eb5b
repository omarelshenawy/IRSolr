{"add":{"doc":{"id":"f6ef3598755bbff70504afe85604651cd86859958466399ac5a167c22c48eb5b","url":"https://upload.wikimedia.org/math/a/1/c/a1ccd1a62bdb39f330a4e4fa62759677.png","previous":["The special case of information entropy for a random variable with two outcomes is the binary entropy function, usually taken to the logarithmic base 2, thus having the shannon (Sh) as unit:","(Here, is the self-information, which is the entropy contribution of an individual message, and is the expected value.) A property of entropy is that it is maximized when all the messages in the message space are equiprobable ,—i.e., most unpredictable—in which case ."],"after":["The joint entropy of two discrete random variables and is merely the entropy of their pairing: . This implies that if and are independent, then their joint entropy is the sum of their individual entropies."]}}}