{"add":{"doc":{"id":"e565f8090079438c6ba23ffb982e0744662d14b289b969a2f5b54204a32c5ccc","url":"https://upload.wikimedia.org/math/6/1/3/6139d5a57705b14d90b05781be46676f.png","previous":["The Kullback–Leibler divergence (or information divergence, information gain, or relative entropy) is a way of comparing two distributions: a \"true\" probability distribution p(X), and an arbitrary probability distribution q(X). If we compress data in a manner that assumes q(X) is the distribution underlying some data, when, in reality, p(X) is the correct distribution, the Kullback–Leibler divergence is the number of average additional bits per datum necessary for compression. It is thus defined"],"after":["Although it is sometimes used as a 'distance metric', KL divergence is not a true metric since it is not symmetric and does not satisfy the triangle inequality (making it a semi-quasimetric)."]}}}