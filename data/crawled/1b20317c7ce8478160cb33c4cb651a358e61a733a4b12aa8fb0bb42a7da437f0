{"add":{"doc":{"id":"1b20317c7ce8478160cb33c4cb651a358e61a733a4b12aa8fb0bb42a7da437f0","url":"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/R._A._Fischer.jpg/230px-R._A._Fischer.jpg","previous":["The modern field of statistics emerged in the late 19th and early 20th century in three stages.[27] The first wave, at the turn of the century, was led by the work of Sir Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions to the field included introducing the concepts of standard deviation, correlation, regression and the application of these methods to the study of the variety of human characteristics â€“ height, weight, eyelash length among others.[28] Pearson developed the Correlation coefficient, defined as a product-moment,[29] the method of moments for the fitting of distributions to samples and the Pearson's system of continuous curves, among many other things.[30] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biometry, and the latter founded the world's first university statistics department at University College London.[31]"],"after":["The second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Sir Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1916 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance and his classic 1925 work Statistical Methods for Research Workers. His paper was the first to use the statistical term, variance. He developed rigorous experimental models and also originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.[32]"]}}}