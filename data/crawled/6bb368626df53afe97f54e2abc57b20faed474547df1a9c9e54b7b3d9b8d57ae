{"add":{"doc":{"id":"6bb368626df53afe97f54e2abc57b20faed474547df1a9c9e54b7b3d9b8d57ae","url":"https://upload.wikimedia.org/math/a/7/9/a79044209a8ae6976a20110235bb4e89.png","previous":["Mutual information is symmetric:"],"after":["Mutual information can be expressed as the average Kullbackâ€“Leibler divergence (information gain) between the posterior probability distribution of X given the value of Y and the prior distribution on X:"]}}}