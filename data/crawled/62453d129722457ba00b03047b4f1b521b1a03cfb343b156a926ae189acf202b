{"add":{"doc":{"id":"62453d129722457ba00b03047b4f1b521b1a03cfb343b156a926ae189acf202b","url":"https://upload.wikimedia.org/math/9/5/f/95f35b63d6e42c137c3d8b9aa971230d.png","previous":["In other words, this is a measure of how much, on the average, the probability distribution on X will change if we are given the value of Y. This is often recalculated as the divergence from the product of the marginal distributions to the actual joint distribution:"],"after":["Mutual information is closely related to the log-likelihood ratio test in the context of contingency tables and the multinomial distribution and to Pearson's Ï‡2 test: mutual information can be considered a statistic for assessing independence between a pair of variables, and has a well-specified asymptotic distribution."]}}}