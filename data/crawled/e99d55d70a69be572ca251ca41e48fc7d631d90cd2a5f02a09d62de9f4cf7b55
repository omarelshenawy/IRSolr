{"add":{"doc":{"id":"e99d55d70a69be572ca251ca41e48fc7d631d90cd2a5f02a09d62de9f4cf7b55","url":"https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/WebCrawlerArchitecture.svg/300px-WebCrawlerArchitecture.svg.png","previous":["A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes."],"after":["A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture.","Shkapenyuk and Suel noted that:[39]"]}}}