{"add":{"doc":{"id":"78be2c3160976e33acfb91277356466d6d9182babf7968c2699e1d844f7a9940","url":"https://upload.wikimedia.org/math/8/c/9/8c9a47120e2cf50d9600cbdfe5f3907f.png","previous":["Mutual information can be expressed as the average Kullbackâ€“Leibler divergence (information gain) between the posterior probability distribution of X given the value of Y and the prior distribution on X:"],"after":["In other words, this is a measure of how much, on the average, the probability distribution on X will change if we are given the value of Y. This is often recalculated as the divergence from the product of the marginal distributions to the actual joint distribution:"]}}}