{
	"add": {
		"doc": {
			"id": "8bcf308c03b58e5f3ccc47595cf2448a791cce010552f185a952a6dd0c962ca4",
			"url": "https://upload.wikimedia.org/math/e/5/e/e5e4f67c8cf506a16dfe36b328b30fed.png",
				"previous": "In what follows, an expression of the form is considered by convention to be equal to zero whenever This is justified because for any logarithmic base.",
			"after": [
				"The entropy, , of a discrete random variable is a measure of the amount of uncertainty associated with the value of .",
				"Suppose one transmits 1000 bits (0s and 1s). If the value of each these bits is known to the receiver (has a specific value with certainty) ahead of transmission, it is clear that no information is transmitted. If, however, each bit is independently equally likely to be 0 or 1, 1000 shannons of information (also often called bits, in the information theoretic sense) have been transmitted. Between these two extremes, information can be quantified as follows. If is the set of all messages that could be, and is the probability of some , then the entropy, , of is defined:[8]"
			],
			"color": "gray|0.35275 grey|0.35275 dim|0.30239 gray|0.30239 dim|0.30239 grey|0.30239 dark|0.19203 gray|0.19203 dark|0.19203 grey|0.19203 silver|0.079984 light|0.036661 gray|0.036661 light|0.036661 grey|0.036661 gainsboro|0.024143 white|0.0034453 smoke|0.0034453  "
		}
	}
}
