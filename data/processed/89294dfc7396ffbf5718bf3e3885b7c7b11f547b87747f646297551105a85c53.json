{
	"add": {
		"doc": {
			"id": "89294dfc7396ffbf5718bf3e3885b7c7b11f547b87747f646297551105a85c53",
			"url": "https://upload.wikimedia.org/math/4/e/0/4e05e522a4e0e14c7324262acb17d5f4.png",
				"previous": "Suppose one transmits 1000 bits (0s and 1s). If the value of each these bits is known to the receiver (has a specific value with certainty) ahead of transmission, it is clear that no information is transmitted. If, however, each bit is independently equally likely to be 0 or 1, 1000 shannons of information (also often called bits, in the information theoretic sense) have been transmitted. Between these two extremes, information can be quantified as follows. If is the set of all messages that could be, and is the probability of some , then the entropy, , of is defined:[8]",
				"after": "The special case of information entropy for a random variable with two outcomes is the binary entropy function, usually taken to the logarithmic base 2, thus having the shannon (Sh) as unit:",
			"color": "gray|0.31625 grey|0.31625 dark|0.25102 gray|0.25102 dark|0.25102 grey|0.25102 dim|0.18915 gray|0.18915 dim|0.18915 grey|0.18915 silver|0.11619 light|0.060403 gray|0.060403 light|0.060403 grey|0.060403 gainsboro|0.042078 white|0.0070229 smoke|0.0070229 linen|0.0032401 lavender|0.0027921 blush|0.0027921 alice|0.0026867 blue|0.0026867 ghost|0.0025257 white|0.0025257  "
		}
	}
}
