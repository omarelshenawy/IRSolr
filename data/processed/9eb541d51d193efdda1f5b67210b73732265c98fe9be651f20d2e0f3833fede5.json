{
	"add": {
		"doc": {
			"id": "9eb541d51d193efdda1f5b67210b73732265c98fe9be651f20d2e0f3833fede5",
			"url": "https://upload.wikimedia.org/math/4/f/0/4f0080f2c78d5b39d6f8ce8dfa076f8e.png",
				"previous": "Suppose one transmits 1000 bits (0s and 1s). If the value of each these bits is known to the receiver (has a specific value with certainty) ahead of transmission, it is clear that no information is transmitted. If, however, each bit is independently equally likely to be 0 or 1, 1000 shannons of information (also often called bits, in the information theoretic sense) have been transmitted. Between these two extremes, information can be quantified as follows. If is the set of all messages that could be, and is the probability of some , then the entropy, , of is defined:[8]",
				"after": "(Here, is the self-information, which is the entropy contribution of an individual message, and is the expected value.) A property of entropy is that it is maximized when all the messages in the message space are equiprobable ,âi.e., most unpredictableâin which case .",
			"color": "dim|0.35545 gray|0.35545 dim|0.35545 grey|0.35545 gray|0.29038 grey|0.29038 dark|0.1827 gray|0.1827 dark|0.1827 grey|0.1827 silver|0.081256 light|0.042064 gray|0.042064 light|0.042064 grey|0.042064 gainsboro|0.028764 white|0.0052067 smoke|0.0052067 linen|0.0031632  "
		}
	}
}
