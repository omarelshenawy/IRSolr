{
	"add": {
		"doc": {
			"id": "62453d129722457ba00b03047b4f1b521b1a03cfb343b156a926ae189acf202b",
			"url": "https://upload.wikimedia.org/math/9/5/f/95f35b63d6e42c137c3d8b9aa971230d.png",
				"previous": "In other words, this is a measure of how much, on the average, the probability distribution on X will change if we are given the value of Y. This is often recalculated as the divergence from the product of the marginal distributions to the actual joint distribution:",
				"after": "Mutual information is closely related to the log-likelihood ratio test in the context of contingency tables and the multinomial distribution and to Pearson's Ï2 test: mutual information can be considered a statistic for assessing independence between a pair of variables, and has a well-specified asymptotic distribution.",
			"color": "gray|0.30807 grey|0.30807 dim|0.26116 gray|0.26116 dim|0.26116 grey|0.26116 dark|0.23052 gray|0.23052 dark|0.23052 grey|0.23052 silver|0.10258 light|0.043735 gray|0.043735 light|0.043735 grey|0.043735 gainsboro|0.030707 white|0.0056583 smoke|0.0056583 linen|0.0029638  "
		}
	}
}
