{
	"add": {
		"doc": {
			"id": "97db074ebc6c33546deaf7e4f2302ef7732935c2990cd5e257ad7abfafa3fb46",
			"url": "https://upload.wikimedia.org/math/8/1/2/8128cfb614ebb99bdbf62bea3d96a912.png",
				"previous": "Suppose one transmits 1000 bits (0s and 1s). If the value of each these bits is known to the receiver (has a specific value with certainty) ahead of transmission, it is clear that no information is transmitted. If, however, each bit is independently equally likely to be 0 or 1, 1000 shannons of information (also often called bits, in the information theoretic sense) have been transmitted. Between these two extremes, information can be quantified as follows. If is the set of all messages that could be, and is the probability of some , then the entropy, , of is defined:[8]",
				"after": "The special case of information entropy for a random variable with two outcomes is the binary entropy function, usually taken to the logarithmic base 2, thus having the shannon (Sh) as unit:",
			"color": "dim|0.37002 gray|0.37002 dim|0.37002 grey|0.37002 gray|0.30403 grey|0.30403 dark|0.17388 gray|0.17388 dark|0.17388 grey|0.17388 silver|0.074146 light|0.036587 gray|0.036587 light|0.036587 grey|0.036587 gainsboro|0.025756 white|0.0043793 smoke|0.0043793  "
		}
	}
}
