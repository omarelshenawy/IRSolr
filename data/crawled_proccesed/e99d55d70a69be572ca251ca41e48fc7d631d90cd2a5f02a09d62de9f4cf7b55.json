{
	"add": {
		"doc": {
			"id": "e99d55d70a69be572ca251ca41e48fc7d631d90cd2a5f02a09d62de9f4cf7b55",
			"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/WebCrawlerArchitecture.svg/300px-WebCrawlerArchitecture.svg.png",
			"previous": " A parallel crawler is a crawler that runs multiple processes in parallel The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page To avoid downloading the same page more than once the crawling system requires a policy for assigning the new URLs discovered during the crawling process as the same URL can be found by two different crawling processes ",
			"after": " A crawler must not only have a good crawling strategy as noted in the previous sections but it should also have a highly optimized architecture  Shkapenyuk and Suel noted that 39 ",
			"after_weights": " A|1 crawler|0.96875 must|0.9375 not|0.90625 only|0.875 have|0.84375 a|0.8125 good|0.78125 crawling|0.75 strategy|0.71875 as|0.6875 noted|0.65625 in|0.625 the|0.59375 previous|0.5625 sections|0.53125 but|0.5 it|0.46875 should|0.4375 also|0.40625 have|0.375 a|0.34375 highly|0.3125 optimized|0.28125 architecture|0.25 |0.21875 Shkapenyuk|0.1875 and|0.15625 Suel|0.125 noted|0.09375 that|0.0625 39|0.03125 |0",
			"previous_weights": " A|0 parallel|0.013889 crawler|0.027778 is|0.041667 a|0.055556 crawler|0.069444 that|0.083333 runs|0.097222 multiple|0.11111 processes|0.125 in|0.13889 parallel|0.15278 The|0.16667 goal|0.18056 is|0.19444 to|0.20833 maximize|0.22222 the|0.23611 download|0.25 rate|0.26389 while|0.27778 minimizing|0.29167 the|0.30556 overhead|0.31944 from|0.33333 parallelization|0.34722 and|0.36111 to|0.375 avoid|0.38889 repeated|0.40278 downloads|0.41667 of|0.43056 the|0.44444 same|0.45833 page|0.47222 To|0.48611 avoid|0.5 downloading|0.51389 the|0.52778 same|0.54167 page|0.55556 more|0.56944 than|0.58333 once|0.59722 the|0.61111 crawling|0.625 system|0.63889 requires|0.65278 a|0.66667 policy|0.68056 for|0.69444 assigning|0.70833 the|0.72222 new|0.73611 URLs|0.75 discovered|0.76389 during|0.77778 the|0.79167 crawling|0.80556 process|0.81944 as|0.83333 the|0.84722 same|0.86111 URL|0.875 can|0.88889 be|0.90278 found|0.91667 by|0.93056 two|0.94444 different|0.95833 crawling|0.97222 processes|0.98611 |1"
		}
	}
}
