{
	"add": {
		"doc": {
			"id": "797de122d8d25991be6385ef98c886553d2f02ea58fdc6fcfd45b408fca9fd19",
			"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/7/75/Linear_least_squares%282%29.svg/220px-Linear_least_squares%282%29.svg.png",
			"previous": " Mean squared error is used for obtaining efficient estimators a widely used class of estimators Root mean square error is simply the square root of mean squared error ",
			"after": " Many statistical methods seek to minimize the residual sum of squares and these are called methods of least squares in contrast to Least absolute deviations The later gives equal weight to small and big errors while the former gives more weight to large errors Residual sum of squares is also differentiable which provides a handy property for doing regression Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non linear least squares Also in a linear regression model the non deterministic part of the model is called error term disturbance or more simply noise Both linear regression and non linear regression are addressed in polynomial least squares which also describes the variance in a prediction of the dependent variable y axis as a function of the independent variable x axis and the deviations errors noise disturbances from the estimated fitted curve ",
			"after_weights": " Many|1 statistical|0.99346 methods|0.98693 seek|0.98039 to|0.97386 minimize|0.96732 the|0.96078 residual|0.95425 sum|0.94771 of|0.94118 squares|0.93464 and|0.9281 these|0.92157 are|0.91503 called|0.9085 methods|0.90196 of|0.89542 least|0.88889 squares|0.88235 in|0.87582 contrast|0.86928 to|0.86275 Least|0.85621 absolute|0.84967 deviations|0.84314 The|0.8366 later|0.83007 gives|0.82353 equal|0.81699 weight|0.81046 to|0.80392 small|0.79739 and|0.79085 big|0.78431 errors|0.77778 while|0.77124 the|0.76471 former|0.75817 gives|0.75163 more|0.7451 weight|0.73856 to|0.73203 large|0.72549 errors|0.71895 Residual|0.71242 sum|0.70588 of|0.69935 squares|0.69281 is|0.68627 also|0.67974 differentiable|0.6732 which|0.66667 provides|0.66013 a|0.65359 handy|0.64706 property|0.64052 for|0.63399 doing|0.62745 regression|0.62092 Least|0.61438 squares|0.60784 applied|0.60131 to|0.59477 linear|0.58824 regression|0.5817 is|0.57516 called|0.56863 ordinary|0.56209 least|0.55556 squares|0.54902 method|0.54248 and|0.53595 least|0.52941 squares|0.52288 applied|0.51634 to|0.5098 nonlinear|0.50327 regression|0.49673 is|0.4902 called|0.48366 non|0.47712 linear|0.47059 least|0.46405 squares|0.45752 Also|0.45098 in|0.44444 a|0.43791 linear|0.43137 regression|0.42484 model|0.4183 the|0.41176 non|0.40523 deterministic|0.39869 part|0.39216 of|0.38562 the|0.37908 model|0.37255 is|0.36601 called|0.35948 error|0.35294 term|0.34641 disturbance|0.33987 or|0.33333 more|0.3268 simply|0.32026 noise|0.31373 Both|0.30719 linear|0.30065 regression|0.29412 and|0.28758 non|0.28105 linear|0.27451 regression|0.26797 are|0.26144 addressed|0.2549 in|0.24837 polynomial|0.24183 least|0.23529 squares|0.22876 which|0.22222 also|0.21569 describes|0.20915 the|0.20261 variance|0.19608 in|0.18954 a|0.18301 prediction|0.17647 of|0.16993 the|0.1634 dependent|0.15686 variable|0.15033 y|0.14379 axis|0.13725 as|0.13072 a|0.12418 function|0.11765 of|0.11111 the|0.10458 independent|0.098039 variable|0.091503 x|0.084967 axis|0.078431 and|0.071895 the|0.065359 deviations|0.058824 errors|0.052288 noise|0.045752 disturbances|0.039216 from|0.03268 the|0.026144 estimated|0.019608 fitted|0.013072 curve|0.0065359 |0",
			"previous_weights": " Mean|0 squared|0.035714 error|0.071429 is|0.10714 used|0.14286 for|0.17857 obtaining|0.21429 efficient|0.25 estimators|0.28571 a|0.32143 widely|0.35714 used|0.39286 class|0.42857 of|0.46429 estimators|0.5 Root|0.53571 mean|0.57143 square|0.60714 error|0.64286 is|0.67857 simply|0.71429 the|0.75 square|0.78571 root|0.82143 of|0.85714 mean|0.89286 squared|0.92857 error|0.96429 |1"
		}
	}
}
